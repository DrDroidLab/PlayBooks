import json
import logging

from django.conf import settings
from openai import OpenAI
from google.protobuf.wrappers_pb2 import StringValue

from intelligence_layer.task_result_interpreters.metric_task_result_interpreters.utils import \
    metric_source_displace_name_map, generate_graph_for_metric_timeseries_result
from media.utils import generate_local_image_path
from protos.playbooks.intelligence_layer.interpreter_pb2 import Interpretation as InterpretationProto
from protos.playbooks.playbook_pb2 import PlaybookMetricTaskExecutionResult as PlaybookMetricTaskExecutionResultProto, \
    PlaybookTaskDefinition as PlaybookTaskDefinitionProto

logger = logging.getLogger(__name__)


def get_task_result_data_type(task_result: PlaybookMetricTaskExecutionResultProto):
    if task_result.result.type == PlaybookMetricTaskExecutionResultProto.Result.Type.TIMESERIES:
        return 'metric'
    else:
        return None


def gpt_metric_inference(generated_image_url: str):
    keys = """
        anomaly_detected:boolean
        description:string
    """

    system_prompt = """
                    You are a DevOps engineer who is an expert at reading metrics graphs. These metrics are 
                    generated by tools and services that you use to deploy and monitor your applications or services.
                    You are responsible for reading the metrics and detecting if there is any anomaly here.
                    You will be given an image of the metrics data. You will need to return a JSON object with the 
                    following keys:
                    {keys}
        """.format(keys=keys)

    gpt_prompt = [
        {
            "type": "text",
            "text": "This image is of and identify if you find any anomaly. If you do, describe the issue. Only send "
                    "the keys that are requested. Do not send more keys or keys with empty or N/A values. Strictly "
                    "do not respond with empty strings or N/A or 'No Data' as key values."
        },
        {
            "type": "image_url",
            "image_url": {
                "url": generated_image_url,
            },
        },
    ]

    open_ai_key = settings.OPENAI_API_KEY
    if not open_ai_key:
        raise ValueError('OpenAI API key is not set.')
    client = OpenAI(api_key=open_ai_key)

    response = client.chat.completions.create(
        model="gpt-4-turbo",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": gpt_prompt}
        ]
    )

    logger.info(response.choices[0])
    logger.info(response.choices[0].message.content)

    return response.choices[0].message.content


def vision_api_evaluation_function(data_type, image_url: str):
    if data_type == 'metric':
        try:
            gpt_response = gpt_metric_inference(image_url)
        except Exception as e:
            logger.error(f'Error getting GPT response: {e}')
            raise e
        gpt_response = json.loads(gpt_response)
        return {'anomaly_detected': gpt_response['anomaly_detected'], 'description': gpt_response['description']}
    # elif data_type == 'logs':
    #     inference_df = pd.DataFrame(columns=['anomaly_detected', 'description', 'additional_data'])
    #
    # elif data_type == 'db_query':
    #     inference_df = pd.DataFrame(columns=['anomaly_detected', 'description', 'additional_data'])
    #
    # elif data_type == 'deployments':
    #     inference_df = pd.DataFrame(columns=['anomaly_detected', 'description', 'additional_data'])
    #
    # elif data_type == 'container_logs':
    #     inference_df = pd.DataFrame(columns=['anomaly_detected', 'description', 'additional_data'])
    else:
        logger.error(f'Unsupported data type: {data_type}')
        raise NotImplementedError(f'Unsupported data type: {data_type}')


def llm_chat_gpt_vision_metric_task_result_interpreter(task: PlaybookTaskDefinitionProto,
                                                       task_result: PlaybookMetricTaskExecutionResultProto) -> InterpretationProto:
    if not settings.OPENAI_API_KEY:
        raise ValueError('OpenAI API key is not set.')
    file_key = generate_local_image_path()
    metric_expression = task_result.metric_expression.value
    metric_expression = metric_expression.replace('`', '')
    metric_name = task_result.metric_name.value
    metric_source = metric_source_displace_name_map.get(task_result.metric_source)
    result = task_result.result
    result_type = result.type
    data_type = get_task_result_data_type(task_result)
    if not data_type:
        raise NotImplementedError('Chat GPT Vision interpreter not supported for task result')
    if result_type == PlaybookMetricTaskExecutionResultProto.Result.Type.TIMESERIES:
        try:
            image_url = generate_graph_for_metric_timeseries_result(result, file_key, task.name.value)
        except Exception as e:
            logger.error(f'Error generating graph using metric timeseries data: {e}')
            raise e
    else:
        raise NotImplementedError(f'Unsupported result type: {result_type}')
    try:
        inference = vision_api_evaluation_function(data_type, image_url)
    except Exception as e:
        logger.error(f'Error evaluating task: {e}')
        raise e
    if not inference or inference is None:
        raise ValueError('No inference returned from chat gpt')

    title = f'Fetched `{metric_expression}` for `{metric_name}` from `{metric_source}`'
    description = f'Description: {inference["description"]}'
    summary = f'Anomaly Detected: {inference["anomaly_detected"]}'

    return InterpretationProto(
        type=InterpretationProto.Type.IMAGE,
        title=StringValue(value=title),
        description=StringValue(value=description),
        summary=StringValue(value=summary),
        image_url=StringValue(value=image_url),
    )
